{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code influenced by \n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\n",
    "import numpy as np\n",
    "from pprint import pprint# Look at parameters used by our current forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 5, stop = 40, num = 25)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(2, 100, num = 99)]\n",
    "print(max_depth)\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid_rf = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "random_grid_dt = {'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "                }\n",
    "\n",
    "random_grid_mlp = {\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideal parameter range for random forest:\n",
    "\n",
    "best_params_rf = {'bootstrap': True, 'max_depth': 15, 'max_features': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 15}\n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [int(x) for x in np.linspace(1, 40, num = 40)],\n",
    "        'max_features': [2, 3],\n",
    "        'min_samples_leaf': [1, 2, 3, 4],\n",
    "        'min_samples_split': [0.5, 2, 3],\n",
    "        'n_estimators': [int(x) for x in np.linspace(1, 40, num = 40)]\n",
    "    }\n",
    "\n",
    "# Ideal parameter range for decision tree\n",
    "best_params_dt = {'max_depth': 21, 'max_features': 3, 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
    "param_grid_dt = {'min_samples_split': [2, 3, 4, 5], \n",
    "              'min_samples_leaf': [1, 2, 3, 4], \n",
    "              'max_features': [2, 3],\n",
    "              'max_depth': [int(x) for x in np.linspace(1, 60, num = 30)]}\n",
    "\n",
    "# Ideal parameter range for KNN\n",
    "best_params_knn = {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 11, 'weights': 'distance'}\n",
    "param_grid_knn = {\n",
    "                'n_neighbors': [int(x) for x in np.linspace(1, 50, num = 50)],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                'leaf_size': [int(x) for x in np.linspace(1, 50, num = 50)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def makeRandomClassifier(df, labels):\n",
    "    n_est = 15\n",
    "    depth = 15\n",
    "    name = 'RandomForest d=' + str(depth) + ' n_est=' + str(n_est)\n",
    "    classifier = RandomForestClassifier(bootstrap: True, max_depth: 15, max_features: 3, min_samples_leaf: 1, min_samples_split: 2, n_estimators: 15)\n",
    "    classifier2 = DecisionTreeClassifier(max_depth= 21, max_features= 3, min_samples_leaf= 1, min_samples_split= 3)\n",
    "    classifier3 = KNeighborsClassifier(algorithm= 'auto', leaf_size= 1, n_neighbors= 11, weights= 'distance')\n",
    "    classifier4 = GaussianNB()\n",
    "    \n",
    "    devices = df[target].unique()\n",
    "#     df = pd.read_excel(path)\n",
    "\n",
    "    df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)#Remove unnecessary columns from the dataframe\n",
    "#         df.drop(df.columns[df.columns.str.contains('Bursts',case = False)],axis = 1, inplace = True)#Remove unnecessary columns from \n",
    "#     testRandomClassifiers(df, name, classifier2, labels, random_grid_dt)\n",
    "    GridSearchClassifiers(df, name, classifier3, labels, param_grid_knn)\n",
    "#     makeClassifier(df, name, classifier2, labels)\n",
    "    \n",
    "    # Create the parameter grid based on the results of random search \n",
    "    return devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testRandomClassifiers(train_df, name, classifier, labels, random_grid):\n",
    "    devices = train_df[target].unique()\n",
    "    print(devices)\n",
    "    copydf = train_df.copy()\n",
    "    \n",
    "    copydf[target].replace(devices, range(0, len(devices)), inplace=True)\n",
    "        \n",
    "    Y = copydf[target].tolist()\n",
    "    \n",
    "#     print(\"There are \" + str(len(devices)) + \" devices\")\n",
    "    \n",
    "    #Remove labelling columns from the index\n",
    "    copydf = copydf.drop(columns= labels)\n",
    "    X = copydf.values\n",
    "    \n",
    "    \n",
    "    #One tenth of the data as test\n",
    "    validation_size = 0.1\n",
    "    \n",
    "    seed = 7\n",
    "    \n",
    "    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, \n",
    "                                                    test_size=validation_size, random_state=seed)\n",
    "    \n",
    "    scoring = {'acc': 'accuracy',\n",
    "           'prec_macro': 'precision_macro',\n",
    "           'rec_macro': 'recall_macro',\n",
    "            'f1_macro' : 'f1_macro'}\n",
    "    clf_random = RandomizedSearchCV(estimator = classifier, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "    clf_random.fit(X_train, Y_train)\n",
    "    print(clf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchClassifiers(train_df, name, classifier, labels, param_grid):\n",
    "    devices = train_df[target].unique()\n",
    "    print(devices)\n",
    "    copydf = train_df.copy()\n",
    "    \n",
    "    copydf[target].replace(devices, range(0, len(devices)), inplace=True)\n",
    "        \n",
    "    Y = copydf[target].tolist()\n",
    "    \n",
    "#     print(\"There are \" + str(len(devices)) + \" devices\")\n",
    "    \n",
    "    #Remove labelling columns from the index\n",
    "    copydf = copydf.drop(columns= labels)\n",
    "    X = copydf.values\n",
    "    \n",
    "    \n",
    "    #One tenth of the data as test\n",
    "    validation_size = 0.1\n",
    "    \n",
    "    seed = 7\n",
    "    \n",
    "    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, \n",
    "                                                    test_size=validation_size, random_state=seed)\n",
    "    \n",
    "    scoring = {'acc': 'accuracy',\n",
    "           'prec_macro': 'precision_macro',\n",
    "           'rec_macro': 'recall_macro',\n",
    "            'f1_macro' : 'f1_macro'}\n",
    "    grid_search = GridSearchCV(estimator = classifier, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THaW Project\n",
    "#This program is meant to establish the accuracy of predicting device name from the data collected by MSU\n",
    "#6/12/2019\n",
    "#Code Written By: Manzi Bryan with a lot of help from https://www.kaggle.com/nageshnaik/iris-dataset-classfication-using-naive-bayes\n",
    "# This code should be submitted for review \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import *\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import *\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from path import Path\n",
    "import warnings\n",
    "import pickle\n",
    "import operator\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This method is meant to build a Random Forest Classifier given a dataframe with devices as targets\n",
    "# The dontTrainOn parameter is meant to tell the classifier to leave one tenth of one device out of training\n",
    "\n",
    "# This method is meant to build a Random Forest Classifier given a dataframe with devices as targets\n",
    "\n",
    "def makeClassifier(df, functionName, clf, labels, f1Dataframe = None, i= None,):\n",
    "    \n",
    "    inTestingMode = not (f1Dataframe == None and i == None)\n",
    "    devices = df[target].unique()\n",
    "    print(devices)\n",
    "    copydf = df.copy()\n",
    "    \n",
    "    copydf[target].replace(devices, range(0, len(devices)), inplace=True)\n",
    "        \n",
    "    Y = copydf[target].tolist()\n",
    "    \n",
    "#     print(\"There are \" + str(len(devices)) + \" devices\")\n",
    "    \n",
    "    #Remove labelling columns from the index\n",
    "    copydf = copydf.drop(columns= labels)\n",
    "    X = copydf.values\n",
    "    \n",
    "    \n",
    "    #One tenth of the data as test\n",
    "    validation_size = 0.1\n",
    "    \n",
    "    seed = 7\n",
    "    \n",
    "    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, \n",
    "                                                    test_size=validation_size, random_state=seed)\n",
    "    \n",
    "    scoring = {'acc': 'accuracy',\n",
    "           'prec_macro': 'precision_macro',\n",
    "           'rec_macro': 'recall_macro',\n",
    "            'f1_macro' : 'f1_macro'}\n",
    "    \n",
    "#     UNCOMMENT THIS + LINE 83 OUT TO GENERATE A LOT OF CLASSIFIERS \n",
    "\n",
    "\n",
    "#     param_grid = {\n",
    "#         'bootstrap': [True],\n",
    "#         'max_depth': [int(x) for x in np.linspace(1, 40, num = 40)],\n",
    "#         'max_features': [2, 3],\n",
    "#         'min_samples_leaf': [1, 2, 3, 4],\n",
    "#         'min_samples_split': [0.5, 2, 3],\n",
    "#         'n_estimators': [int(x) for x in np.linspace(1, 40, num = 40)]\n",
    "#     }\n",
    "\n",
    "#     rf = RandomForestClassifier()# Instantiate the grid search model\n",
    "#     grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "#                               cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "#     grid_search.fit(X_train, Y_train)\n",
    "#     print(grid_search.best_params_)\n",
    "    \n",
    "#     Fitting the training set\n",
    "    clf.fit(X_train, Y_train)\n",
    "#     print(clf.best_params_)\n",
    "    \n",
    "#         Model Performance\n",
    "#     setting performance parameters\n",
    "    kfold = model_selection.StratifiedKFold(n_splits=10, random_state=seed) #same number of samples from each \n",
    "\n",
    "    #calling the cross validation function\n",
    "    \n",
    "    cv_results = cross_validate(clf, X_train, Y_train, cv=kfold, scoring=scoring, return_train_score=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    if not inTestingMode:\n",
    "        filename = functionName  + 'Model.sav' # Save Classifier\n",
    "#         pickle.dump(clf, open(filename, 'wb'))\n",
    "        for metric in cv_results.keys():\n",
    "            print(metric + \": \" + str(cv_results[metric].mean()))\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        f1Dataframe['Classifier'][i] = functionName\n",
    "        f1Dataframe['f1'][i] = str(cv_results['test_f1_macro'].mean())\n",
    "    \n",
    "    return clf\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeOneClassifier(df, labels):\n",
    "    n_est = 15\n",
    "    depth = 15\n",
    "    name = 'RandomForest d=' + str(depth) + ' n_est=' + str(n_est)\n",
    "    classifier = RandomForestClassifier(n_estimators=n_est, max_depth=depth)\n",
    "    \n",
    "    crazyClassifier = RandomForestClassifier(n_estimators= 28, min_samples_split= 2, \n",
    "     min_samples_leaf= 1, max_features= 3, max_depth= 33, \n",
    "     bootstrap= True)\n",
    "    \n",
    "\n",
    "    devices = df[target].unique()\n",
    "\n",
    "    df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)#Remove unnecessary columns from the dataframe\n",
    "#         df.drop(df.columns[df.columns.str.contains('Bursts',case = False)],axis = 1, inplace = True)#Remove unnecessary columns from \n",
    "    makeClassifier(df, name, classifier, labels)\n",
    "    makeClassifier(df, name, crazyClassifier, labels)\n",
    "    return devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to read...\n",
      "...read\n",
      "['Gian Iphone(A)' 'Portable ECG moniter(A)'\n",
      " 'Bodimetrics Performance monitor(A)' 'Apple Watch(A)'\n",
      " 'Eko Stethescope(A)' 'iHealth Blood Pressure(A)'\n",
      " 'iHealth Blood Pressure(B)' 'Fever Sense(A)' 'Portable ECG(B)'\n",
      " 'Fever Sense(B)' 'iHealth gluco(A)' 'iHealth gluco(B)' 'Omron V10(A)'\n",
      " 'Pyle Health(B)' 'Pyle Health(A)' 'Portable_ECG(A)' 'Eko Sthethoscope(A)']\n",
      "Fitting 3 folds for each of 20000 candidates, totalling 60000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:   53.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10352 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11689 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 13106 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 14605 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 16184 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 17845 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 19586 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 21409 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 23312 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 25297 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 27362 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 29509 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 31736 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done 34045 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 36434 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done 38905 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=-1)]: Done 41456 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done 44089 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=-1)]: Done 46802 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done 49597 tasks      | elapsed: 24.6min\n",
      "[Parallel(n_jobs=-1)]: Done 52472 tasks      | elapsed: 28.0min\n",
      "[Parallel(n_jobs=-1)]: Done 55429 tasks      | elapsed: 31.5min\n",
      "[Parallel(n_jobs=-1)]: Done 58466 tasks      | elapsed: 35.2min\n",
      "[Parallel(n_jobs=-1)]: Done 60000 out of 60000 | elapsed: 37.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 11, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "if __name__ == \"__main__\":\n",
    "    target = 'Device'\n",
    "    path=r'C:\\Users\\brnma\\train.xlsx'\n",
    "    \n",
    "    print(\"About to read...\")\n",
    "    traindf = pd.read_excel(path) \n",
    "    print(\"...read\")\n",
    "    traindf = traindf.reindex(sorted(traindf.columns), axis=1)\n",
    "    \n",
    "    labels = ['Device','Model','App','Distance']\n",
    "    makeRandomClassifier(traindf, labels)\n",
    "#     print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'activation': 'relu',\n",
      " 'alpha': 0.0001,\n",
      " 'batch_size': 'auto',\n",
      " 'beta_1': 0.9,\n",
      " 'beta_2': 0.999,\n",
      " 'early_stopping': False,\n",
      " 'epsilon': 1e-08,\n",
      " 'hidden_layer_sizes': (100,),\n",
      " 'learning_rate': 'constant',\n",
      " 'learning_rate_init': 0.001,\n",
      " 'max_iter': 200,\n",
      " 'momentum': 0.9,\n",
      " 'n_iter_no_change': 10,\n",
      " 'nesterovs_momentum': True,\n",
      " 'power_t': 0.5,\n",
      " 'random_state': 42,\n",
      " 'shuffle': True,\n",
      " 'solver': 'adam',\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': False,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from pprint import pprint\n",
    "\n",
    "mlp = MLPClassifier(random_state = 42)# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(mlp.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
